---
title: "IntroMachineLearning"
author: "Richard G. Gardiner"
date: "12/11/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Part of this material is gathered from Laurent Gatto's workshop on ["An Introduction to Machine Learning with R"](https://github.com/rgardiner90/IntroMachineLearning.git)

## Intro

There are two main groups of machine learning: supervised and unsupervised.  Supervised machine learning (SML) is broken down into classification, where the DV is categorical, and regression, where the output is numerical.  SML is closely tied with "labeled data" which means each observation has an outcome that we are trying to model.  

Unsupervised machine learning (UML) is used when there are no labels provided. Algorithms take data that doesn't have a layed out structure and tries to create its own structure.  There are also semi-supervised learning which uses a combination of labeled data to inform unlabeled data, and reinforcement learning which the learning algorithm "performs a task using feedback from operating in a real or synthetic environment".

Gatto's workshop uses the following packages in R:

- caret
- ggplot2
- mlbench
- class
- caTools
- randomForest
- impute
- ranger
- kernlab
- class
- glmnet
- naivebayes
- rpart
- rpart.plot


## Example Datasets:

Iris:
```{r}
data(iris) # iris

data(mtcars) # mtcars

# sub-celllar localisation
#https://github.com/lgatto/hyperLOPIT-csvs/


library(tidyverse)
data(diamonds) # diamonds dataset


library(mlbench)
data("Sonar") # used to train a classifer to recognise mines from rocks using sonar data.

library("MASS")
data(Boston) # bostom median home values 

library("C50")
data(churn) # samples of customer attrition
dim(churnTrain) # training data
dim(churnTest) # test data

```



## Unsuperivsed Learning

As mentioned above, unsupervised learning is the most common route when there is no labeled data (which happens a lot), and the algorithm focuses on detecting structures within the unlabelled data.  There are two general types of UML:

- Clustering: find homogeneous subgroups within the data, grouping is based on distance between observations
- Dimensionality reduction: identify pattersn in the features of hte data.   This is usually done to allow the researcher to visualize the data or do pre-processing for supervised learning.

One of the most challenging aspects of UML is that there is no single goal.

### k-means clustering

The goal of k-means clustering is to partion *n* observations into a fixd number of *k* clusters.  The algorithm works to find homogenous clusters given the parameters.

the basic code is:
```{r}
# stats::kmeans(x, centers = 3, nstart = 10)
```
Where `x` is the data matrix, `centers` is the number of clusters, and `nstart` is hte random component that can be repeated a number of times to improve the returned model.  

Let's try with the following:

```{r}
data(iris)
iris_new <- iris[,1:4] # taking out the labeled data
c1 <- kmeans(x = iris_new, 3, nstart = 10) 

ggplot(data = iris_new) +
  geom_point(aes(x = Sepal.Length, y = Petal.Length, colour = factor(c1$cluster)), show.legend = FALSE) 
```

#### How does k-means clustering work?






