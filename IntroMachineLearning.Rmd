---
title: "Intro to Machine Learning"
author: "Richard G. Gardiner"
date: "12/11/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)


```

Part of this material is gathered from Laurent Gatto's workshop on ["An Introduction to Machine Learning with R"](https://github.com/rgardiner90/IntroMachineLearning.git)

## Intro

There are two main groups of machine learning: supervised and unsupervised.  Supervised machine learning (SML) is broken down into classification, where the DV is categorical, and regression, where the output is numerical.  SML is closely tied with "labeled data" which means each observation has an outcome that we are trying to model.  

Unsupervised machine learning (UML) is used when there are no labels provided. Algorithms take data that doesn't have a layed out structure and tries to create its own structure.  There are also semi-supervised learning which uses a combination of labeled data to inform unlabeled data, and reinforcement learning which the learning algorithm "performs a task using feedback from operating in a real or synthetic environment".

Gatto's workshop uses the following packages in R:

- caret
- ggplot2
- mlbench
- class
- caTools
- randomForest
- impute
- ranger
- kernlab
- class
- glmnet
- naivebayes
- rpart
- rpart.plot


## Example Datasets:

Iris:
```{r}
data(iris) # iris

data(mtcars) # mtcars

# sub-celllar localisation
#https://github.com/lgatto/hyperLOPIT-csvs/


library(tidyverse)
data(diamonds) # diamonds dataset


library(mlbench)
data("Sonar") # used to train a classifer to recognise mines from rocks using sonar data.

library("MASS")
data(Boston) # bostom median home values 

library("C50")
data(churn) # samples of customer attrition
dim(churnTrain) # training data
dim(churnTest) # test data

```



## Unsupervised Learning

As mentioned above, unsupervised learning is the most common route when there is no labeled data (which happens a lot), and the algorithm focuses on detecting structures within the unlabeled data.  There are two general types of UML:

- Clustering: find homogeneous subgroups within the data, grouping is based on distance between observations
- Dimensionality reduction: identify pattersn in the features of the data.   This is usually done to allow the researcher to visualize the data or do pre-processing for supervised learning.

One of the most challenging aspects of UML is that there is no single goal.

### k-means clustering

The goal of k-means clustering is to partition *n* observations into a fixed number of *k* clusters.  The algorithm works to find homogenous clusters given the parameters.

the basic code is:
```{r}
# stats::kmeans(x, centers = 3, nstart = 10)
```
Where `x` is the data matrix, `centers` is the number of clusters, and `nstart` is the random component that can be repeated a number of times to improve the returned model.  

Let's try with the following:

```{r}
data(iris)
iris_new <- iris[,1:4] # taking out the labeled data
c1 <- kmeans(x = iris_new, 3, nstart = 10) 

a <- ggplot(data = iris_new) +
  geom_point(aes(x = Sepal.Length, y = Petal.Length, colour = factor(c1$cluster)), show.legend = FALSE) +
  labs(title = "Clusters after using kmeans()")
a
```

#### How does k-means clustering work?

Step 1: initialization: randomly assign class membership

```{r}
set.seed(12) 
init <- sample(3, nrow(iris_new), replace = TRUE)
b <- ggplot(data = iris_new) +
  geom_point(aes(x = Sepal.Length, y = Petal.Length, colour = factor(init)), show.legend = FALSE) +
  labs(title = "k-means random initialization")
```

Step 2: iterate

1 Calculate the center of each subgroup as the average position of all observations in that subgroup.
2 Each observation is then assigned to the group of its nearest center.

It is also possible to stop the algorithm after a certain number of iterations, or once the centers move less than a certain distance:

```{r, fig.height = 4, fig.width=4}
centers <- sapply(1:3, function(i) colMeans(iris_new[init == i, ])) # creates a function that gathers the column mean for each column
centers <- t(centers) # transposes the results

tmp <- dist(rbind(centers, iris_new)) # dist() calculates the distance matrix
tmp <- as.matrix(tmp)[, 1:3] # converts the tmp into a matrix (keeps all rows and columms 1-3)
# it must be 3 columns because we picked 3 clusters earlier


# this one returns the column index where there is a minimum value and appplies does so for every row
# basically this finds out which cluster minimizes the distance between cluster and observation
ki <- apply(tmp, MARGIN = 1, FUN = which.min) 
# margin = 1 applies function to row, margin = 2 applies function to columns. 
# note: when MARGIN = c(1,2) it applies to both rows and columns


# gets rid of the first 3 rows (now it fits with the length of the iris_new length)
# I believe the first 3 is kind of like an identity matrix, which is why we got rid of them
ki <- ki[-(1:3)]

c <- ggplot(data = iris_new) +
  geom_point(aes(x = Sepal.Length, y = Petal.Length, colour = factor(ki)), show.legend = FALSE) +
  labs(title = "Results after doing clustering by hand")

gridExtra::grid.arrange(a, b, c)
```

#### Model Selection

Random initialization can lead to different clustering results.  When k=means is run multiple times, the best outcome (the one with the smallest total within cluster sum of squares) is selected.  The total within sum of squares is calculated as, for each cluster, calculated the squared euclidean distance from observation to the center of the cluster for each observation.  Total all distances.

That lowest within cluster sum of squares is called the local minimum, though we might not be able to get a global minimum.

Run multiple kmeans on the data with more than 1 iteration and see if you get the same results:
```{r}
c2 <- kmeans(x = iris_new, 3, nstart = 10) # iteration = 10
c3 <- kmeans(x = iris_new, 3, nstart = 10) 

# combining the results of the two clusters 
table(c2$cluster, c3$cluster) # gives it different names, but same results
```

Now do the same with only 1 iteration:

```{r}
c4 <- kmeans(x = iris_new, 3, nstart = 1) # iteration = 10
c5 <- kmeans(x = iris_new, 3, nstart = 1) 

# combining the results of the two clusters 
table(c4$cluster, c5$cluster) # gives it different names, but same results
```


#### How to determine the number of clusters

Simple steps:

1 run k-means at different levels of clustering
2 Record the total within sum of squares for each clustering
3 Choose k at the *elbow* position (imagine an arm, find the elbow and that is the best)

The elbow is kind of a soft rule.  To me it seems like where you start getting real diminishing returns

Do clustering ranging from 1-10 and then plot them to spot the elbow:
```{r}
k <- 1:10

clustering <- sapply(k, function(i) {
  cl <- kmeans(x = iris_new, i, nstart = 10)
  cl$tot.withinss
})

elbow <- cbind(k, clustering)
elbow <- as.data.frame(elbow)

ggplot(elbow) +
  geom_line(aes(x = k, y = clustering)) +
  geom_point(aes(x = k, y = clustering)) +
  scale_x_discrete(limits = c(0:10))
```

Looks like it could be 3 or 4 here (though we know that 3 is likely the best)


## Hierarchical Clustering

How does it work?  First it starts by assigning each of the n point its own cluster.  Then it iterates.  First it finds the two nearest clusters and joins the together.  This leads to n-1 clusters.  Then they continue merging clusters until all are grouped into a single cluster.

The results of hierarchical clustering are typically shown along with a dendogram where the distance between the clusters is proportional to the branch length.

How to do this in R?

1 Calculate the distance using `dist`, typically the Euclidean distance.
2 Hierarchical clustering on this distance matrix using hclust

```{r}
iris_dist <- dist(iris[, 1:4])
hcl <- hclust(iris_dist)
hcl

plot(hcl)
```


After we have produced the results, we need to *cut the tree (dendogram)* at a specific height to define the clusters.  In the example above, we could decide to cut 

1 around a specific height, like 1.5.
2 to get a certain number of clusters, k = 2 for instance

All of this is done with the cutree function

```{r}
plot(hcl)
abline(h = 3.9, col = "red")

cutree(hcl, h = 3.9) # h = height
cutree(hcl, k = 3) # k = number of clusters

identical(cutree(hcl, h = 3.9), cutree(hcl, k = 3))
```

Now, using the `k = 3` value, verify if k-means and hierarchical clustering produce the same results on the `iris` dataset.

```{r}
km <- kmeans(iris[,1:4], center = 3, nstart = 10)
hcl <- hclust(dist(iris[,1:4]))

table(km$cluster, cutree(hcl, k =3)) # get some difference with 2 and 3 groupings
```


Now let's graph the two:
```{r}
d <- ggplot(data = iris) +
  geom_point(aes(x = Sepal.Length, y = Petal.Length, colour = factor(km$cluster)), show.legend = FALSE) +
  labs(title = "k-means")

e <- ggplot(data = iris) +
  geom_point(aes(x = Sepal.Length, y = Petal.Length, colour = factor(cutree(hcl, k = 3))), show.legend = FALSE)  +
  labs(title = "Hierarchical")

gridExtra::grid.arrange(d, e)
```


We can see that the hierarchical has a much larger group 3 than the k-means, but let's do a simple check (since we have labeled data)

```{r}
table(iris$Species, km$cluster) # does really good with the first cluster, okay with vericolor, and not great with virginica
```

```{r}
table(iris$Species, cutree(hcl, k = 3)) # this does really well with setosa and virginica, but poorly with veriscolor
```


### Hierarchical v. K-means

There are obvious advantages for each one.  K-means is generally faster than hierarchical clustering, for instance.  My read of the profession, though,  hierarchical clustering is is favored, all else equal.  Hierarchical doesn't require stating at the beginning how many clusters you want.  You can decide after the fact.  K-means also has a lot of assumptions about the distribution of the data, and is more sensitive to outliers.  

## Pre-processing

Both unsupervised and supervised ML is sensitive to different scales.  A typical way to pre-process the data is to scale it or apply principal component analysis (next section).  Scaling means that all the columns will have a mean of 0 and a standard deviation of 1.  Scaling can be done with the `scale` function.  

Example: use the `scale` function to scale the data in mtcars.  Run hierarchical clustering on both the original and the scaled to see how that changes the output.

```{r}
summary(mtcars) # definitely different means

mtcars_scaled <- scale(mtcars)

summary(mtcars_scaled)
```

```{r}
mtcars_hcl <- hclust(dist(mtcars))


mtcars_scaled_hcl <- hclust(dist(mtcars_scaled))

par(mfrow = c(1, 2))
plot(mtcars_hcl, main = "original")
plot(mtcars_scaled_hcl, main = "scaled")
```


There are clear differences between the two.  As the same suggests, pre-processing is done as a preliminary practice that will make later analysis more efficient.  The method of scaling is only one way to pre-process the data (also called normalization).  Other techniques includes data: cleaning, instance selection, transformation, feature extraction, and selection.  Cleaning is fixing bad values (i.e. men can't be pregnant but the raw data may have that).  Feature extraction will be covered more (I believe) in the Principal Components Analysis section.  


## Principal Component Analysis (PCA)

PCA is widely used (and considered very versitile) that is used for: finding structure in features, pre-processing, and aiding in visualization.  The basic idea behind dimensionality reduction is to transform the data into a new space that "summarise properties of the whole dataset along a reduced number of dimensions.  These are the nidela candidates to visualize the data laong these reduced number of informative dimensions."

### How Does it work?

PCA is a technique that transforms the original n-dimensional data into a new n-dimensional space.

- These new dimension are linear combinations of the original data (meaning they are composed of proportions of the original variables)
- Along these new dimensions, called **principal components**, the data expresses most of its variablility along the first PC, then second, ...
- Principal components are orthogonal to each other (not correlated).


#### Example

using the iris data again, we can see that while it only contains 4 variables, it is already confusing to visualize the three groups along these dimensions (never mind the dozen+ variables that are common in research).

```{r}
plot(iris[, -5], col = iris$Species)
```


We can use `prcomp` to reduce the number of dimensions:

```{r}
iris_pca <- prcomp(iris[, -5]) # not doing this with the species variable

summary(iris_pca)
```

This summary shows us that along PC1 alone, we are able to retain 92% of the total variability in the data.

Now we can visualize the results.  A biplot shows all of the original points re=mapped (rotated) along the first two PCs as well as the original features as vectors along the same PCs.  Feature vectors that are in the same direction in PC space are also correlated in the original data space.  

```{r}
biplot(iris_pca)
```


NOTE: one important piece of PCa is that proportion of vairance explained along the PCs, in particular when dealing with high dimensional data, as PC1 and PC2 (which are generally used for visualization), might only account for an insufficient proportion of variance to be relevant on their own.  

We can extract the standard deviations from the PCA result to calculated the variances, then obtain a percentage of and cumulative variance along the PCs.

```{r}
var <- iris_pca$sdev^2

pve <- var/sum(var)
pve

cumsum(pve) 
```


```{r}
par(mfrow = c(1, 2))
plot(iris_pca$x[, 1:2], col = iris$Species)
plot(iris_pca$x[, 3:4], col = iris$Species)
```

### Notes on PCA

PCA cannot deal with missing values which means it will simply drop those values.  That may not be a problem if the proportion of missing data is small, but it should be considered.  Second, be careful of using UML with categorical data because the computer will generally assume the intenger provided to represent the category actually has some meaning (i.e., there is ordering and distance matters). You should either drop them before doing UML or create dummy variables.



## t-Distributed Stochastic Neighbor Embedding


The t-Distributed Stochastic Neighbour Embedding (t-SNE) is a non-linear dimensionality reduction technique, that different regions of the data space will be subjected to different transformations.  T-SNE will compress small distances, thus bringing close neighbr together, and will ignore distances.  **It is particularly well suited for very high dimensional data**. t-SNE is especially popular in machine learning because it is very effective at creating impressive two-dimensional "maps" from data with hundreds or even thousands of dimensions, [see this link for more information](https://distill.pub/2016/misread-tsne/). 

In R, we can use the `Rtsne` function from the `Rtsne` package.  First, though, we have to remove any duplicates.

```{r}
library(Rtsne)

unique_iris <- unique(iris[, 1:5])

irist_tsne <- Rtsne(unique_iris[, 1:4])

plot(irist_tsne$Y, col = unique_iris$Species)
```

If we wanted to, we could have centered and scaled the data before running the t-SNE function (see the `pca_center` and `pca_scale` arguments).  This algorithm is stochastic, and will produce different results for each repitition.

### Parameter tuning

While t-SNE is popular, it can be confusing to properly set and interpret the model.  Part of this comes from the parameter tuning.  t-SNE have two important parameters that can substantially influence the clustering of data:

- Perplexity: balancing global and local aspects of the data
- Iterations: number of iterations before the clustering is stopped.

Let's see what happens when we change these parameters a bit
```{r}
iris_tsne1 <- Rtsne(unique_iris[, 1:4], perplexity = 5)
iris_tsne2 <- Rtsne(unique_iris[, 1:4], perplexity = 10)
iris_tsne3 <- Rtsne(unique_iris[, 1:4], perplexity = 30)

# different levels of perplexity
par(mfrow = c(2, 2))
plot(iris_tsne1$Y, col = unique_iris$Species)
plot(iris_tsne2$Y, col = unique_iris$Species)
plot(iris_tsne3$Y, col = unique_iris$Species)
```

Now that we have seen what happens when you change the perplexity, let's see what happens when you change the number of iterations for the one that seemed to perform the best (perplexity of 30)
```{r}
iris_tsne4 <- Rtsne(unique_iris[, 1:4], perplexity = 30, max_iter = 10)
iris_tsne5 <- Rtsne(unique_iris[, 1:4], perplexity = 30, max_iter = 1000)
iris_tsne6 <- Rtsne(unique_iris[, 1:4], perplexity = 30, max_iter = 10000)

# different levels of perplexity
par(mfrow = c(2, 2))
plot(iris_tsne4$Y, col = unique_iris$Species)
plot(iris_tsne5$Y, col = unique_iris$Species)
plot(iris_tsne6$Y, col = unique_iris$Species)
```

An iteration of 10 is clearly not enough (the default is 1000), but the 1000 and 10000 ones seem to be pretty good.  At this point it appears to be a matter of preference.  Let's try also applying `pca_center` and `pca_scale` to it (note: pca_center is true by default).

```{r}
iris_tsne7 <- Rtsne(unique_iris[, 1:4], perplexity = 30, max_iter = 10000, pca_center = TRUE, pca_scale = TRUE)

par(mfrow = c(2, 1))
plot(iris_tsne6$Y, col = unique_iris$Species)
plot(iris_tsne7$Y, col = unique_iris$Species)
```

Scaling does reduce the range in the x and y axes which is nicer, but it probably doesn't change this too much.  Clearly the moral of this section is to play around with your parameters before deciding on a specific level.

```{r}
plot(iris_tsne7$Y, col = unique_iris$Species)
```




## Supervised Learning

The difference between Unsupervised machine learning (UML) versus Supervised machine learning (SML) is that SML has labeled example inputs, which indicate the desired output (basically the DV).  There are two main groups of Supervised Machine Learning:

- Classification: the output is qualitative
- Regression: the output is quantitative

### K nearest neighbors (kNN)

Let's use a common SML algorithm, k nearest neighbors, to classify the iris flower dataset.  To do this you will use the `knn` function from the class package.  KNN works by directly measuring the euclidean distance between observations and infer the class of unlabeled data from the class of its nearest neighbor.

In most ML processes there are two steps: train and predict.  In KNN these two steps are combined into a single function called `knn`.  

Let's draw a set of 50 random iris observations to train the model and predict the species of another set of 50 randomly chosen flowers.  The `knn` function takes the training data, the new data (to be inferred) and the labels of the training data, and returns (be default) the predicted class.

```{r}
set.seed(12)

train <- sample(150, 50)
new <- sample(150, 50)

library("class")

knnres <- knn(iris[train, -5], iris[new, -5], iris$Species[train])
head(knnres)

```

Now we can compare the observed KNN-predicted class and the expected known outcome and calculate the over accuracy of the model.

```{r}
table(knnres, iris$Species[new])
```

```{r}
mean(knnres == iris$Species[new])
```


Both of these show that the model does a pretty great job of classifying the data.  

One key argument we have omitted from the `knn` function is the parameter *k*.  The value *k* defines how many nearest neighbors will be considered to assign a class to a new unlabeled observation.  We can look at the arguments for a function by doing the following:

```{r}
args(knn)
```

As we can see, the default for *k* is 1.  But is this a good value?  Would we prefer to look at more neighbors and infer the new class using a vote based on more lables?

#### Challenge

run the knn again but with a different value for k.  Use the same data as before to avoid any biases in comparison

```{r}
knnres_2 <- knn(iris[train, -5], iris[new, -5], iris$Species[train], k = 5)
```

Now let's see how this new model did compared to the old.
```{r}
table(knnres_2, iris$Species[new])

mean(knnres_2 == iris$Species[new])
```

A k = 5 did slightly worse than our k = 1.  Now lets try the same model, but with a `prob = TRUE` to obtain the proportion of the votes for the winning class:

```{r}
knnres_3 <- knn(iris[train, -5], iris[new, -5], iris$Species[train], k = 5, prob = TRUE)

table(attr(knnres_3, "prob"))
```


## Model Performance

In supervised machine learning we have the outcome and thus know what we want to compute.  This is helpful in quantifying how well our model performs.  For regression, we commonly use the *root mean squared error* (RMSE) which is what linear regression (lm) seeks to minimize.  For classification, we will use the model prediction accuracy.

In most cases we don't want to calculate these metrics using the same observations we used to calculate the model.  This approach, *in-sample error*, leads to an overly optimistic assessment of our model.  Instead we prefer *out-of-sample error* which is new done on new data (training/testing data).  We will use the `caret` (Classification And REgression Training) package for most of this.

The code chunk below uses the `lm` function on the diamonds dataset.

```{r}
library(caret)
library(modelr)

model <- lm(price ~ ., diamonds)

# tidy formatted way
model_pred <- diamonds %>%
  add_predictions(model) %>%
  add_residuals(model)
  
rmse_in <- sqrt(mean((model_pred$pred - model_pred$price)^2))
rmse_in
```

Now let's repeat the exercise, but calculate the out-of-sample RMSe.  To do this we need to do a 80/20 split of the data, using 80% to to fit our model to predict the target variable (this is called the training data), the price, on the 20% unseen data (the testing data).  

```{r}
set.seed(42)
ntest <- nrow(diamonds) * .8
test <- sample(nrow(diamonds), ntest)
model2 <- lm(price ~., data = diamonds[test, ])

p <- predict(model, diamonds[-test, ])
error <- p - diamonds$price[-test]

rmse_out <- sqrt(mean(error^2))
rmse_out
```

This model shows that our rmse_in was slightly optimistic, though not much so.  This is likely because of the larger size of the diamonds dataset.  In smaller models, the presence of a single outlier can dramatically change the RMSE.  In that case, we need a more robust method.








```{r}
library(spelling)
spell_check_files("IntroMachineLearning.Rmd")
```